# -*- coding: utf-8 -*-
"""
Created on Mon Mar 19 11:06:27 2018

@author: Yuan-Ray Chang
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import mglearn

from numpy import loadtxt
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import Ridge
# Data processing
initial_data = loadtxt("data_initial.txt")
#print("Initial data:\n{}".format(initial_data))
print("Initial data.shape: {}\n".format(initial_data.shape))
initial_target = np.genfromtxt("area_initial.csv", \
                  skip_header=1, usecols=[1], delimiter=',')
#print("Initial area:\n{}".format(initial_target))
print("Initial area.shape: {}\n".format(initial_target.shape))

usecols = range(1,12)
pso = loadtxt("position_area.txt", usecols=usecols)
#print("Data generated by PSO:\n{}".format(pso))
print("PSO.shape: {}\n".format(pso.shape))

pso = pso[pso[:, 10] !=40]
#print("Data eliminated failed particles:\n{}".format(pso))
print("PSO.shape after elimination: {}\n".format(pso.shape))

pso_data = pso[:, :10]
#print("PSO data:\n{}".format(pso_data))
print("PSO_data.shape: {}\n".format(pso_data.shape))

pso_target = pso[:, 10]
#print("PSO target:\n{}".format(pso_target))
print("PSO_target.shape: {}\n".format(pso_target.shape))

data_all = np.concatenate((initial_data, pso_data), axis=0)
print("data_all.shape: {}\n".format(data_all.shape))
target_all = np.concatenate((initial_target, pso_target), axis=0)
print("target_all.shape: {}\n".format(target_all.shape))

X_train, X_test, y_train, y_test = train_test_split(data_all, \
            target_all, random_state=0)

# Start using DecisionTreeRegressor 
tree = DecisionTreeRegressor(max_depth=18, min_samples_split=2, \
                             min_samples_leaf=1, random_state=1)
tree.fit(X_train, y_train)
print("DecisionTreeRegressor:")
print("Training set score: {}".format(tree.score(X_train, y_train)))
print("Test set score: {}\n".format(tree.score(X_test, y_test)))
#print("Number of features: {}".format(tree.n_features_))
#print("Feature importances: {}".format(tree.feature_importances_ ))


# Start using RandomForestRegressor
print("RandomForestRegressor:")
forest = RandomForestRegressor(n_estimators=2500, random_state=0,\
                               max_features=10, min_samples_split=10, \
                               min_samples_leaf=5, n_jobs=5)
forest.fit(X_train, y_train)
print("Training set score: {}".format(forest.score(X_train, y_train)))
print("Test set score: {}\n".format(forest.score(X_test, y_test)))

# Start using GradientBoostingClassifier
print("GradientBoostingRegressor:")
gbrt = GradientBoostingRegressor(random_state=0)
gbrt.fit(X_train, y_train)
print("Training set score: {}".format(gbrt.score(X_train, y_train)))
print("Test set score: {}\n".format(gbrt.score(X_test, y_test)))

# Start using MLPRegressor
print("MLPRegressor:")
mlp = MLPRegressor(random_state=0, max_iter=1000, \
                   hidden_layer_sizes=[100, 100, 100, 100] )
mlp.fit(X_train, y_train)
print("Training set score: {}".format(mlp.score(X_train, y_train)))
print("Test set score: {}\n".format(mlp.score(X_test, y_test)))

# Start using Ridge
print("Ridge:")
ridge = Ridge().fit(X_train, y_train)
print("Training set score: {}".format(ridge.score(X_train, y_train)))
print("Test set score: {}\n".format(ridge.score(X_test, y_test)))